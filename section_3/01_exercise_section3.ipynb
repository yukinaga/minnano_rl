{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_exercise_section3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr9IjD6Mn5/TQJ37Rbb0gc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/minnano_rl/blob/main/section_3/01_exercise_section3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxam5J6s8Pw9"
      },
      "source": [
        "# 演習: SARSAの実装\n",
        "Section2で暑かったQ学習のコードを、SARSAのコードに変換します。    \n",
        "Brainクラスの記述を変更し、SARSAを実装した上で動作を確認しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iCWKom81u5"
      },
      "source": [
        "## ライブラリの導入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfOUkvmVAmmY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import animation, rc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAGWByiE9Q1M"
      },
      "source": [
        "## Brainクラス\n",
        "エージェントの頭脳となるクラスです。  \n",
        "以下のセルの指定された箇所にコードを追記し、SARSAを実装しましょう。  \n",
        "\n",
        "Q学習では以下の式が用いられます。  \n",
        "\n",
        "$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\eta\\left(R_{t+1}+\\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_{t}, a_{t})\\right) $$\n",
        "\n",
        "ここで、$a_{t}$は行動、$s_t$は状態、$Q(s_t,a_t) $はQ値、$\\eta$は学習係数、$R_{t+1}$は報酬、$\\gamma$は割引率になります。  \n",
        "\n",
        "一方、SARSAでは以下の式が用いられます。。  \n",
        "\n",
        "$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\eta\\left(R_{t+1}+\\gamma Q(s_{t+1}, a_{t+1}) - Q(s_{t}, a_{t})\\right) $$\n",
        "\n",
        "Q学習では$t+1$の時刻における最大のQ値を使いますが、SARSAでは$t+1$において実際に選択した行動のQ値を使います。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7mzk3ujGwjY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_state, w_y, w_vy, n_action, gamma=0.9, r=0.99, lr=0.01):\n",
        "        self.n_state = n_state  # 状態の数\n",
        "        self.w_y = w_y  # 位置の刻み幅\n",
        "        self.w_vy = w_vy  # 速度の刻み幅\n",
        "        self.n_action = n_action  # 行動の数\n",
        "\n",
        "        self.eps = 1.0  # ε\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.r = r  # εの減衰率\n",
        "        self.lr = lr  # 学習係数\n",
        "\n",
        "        self.q_table = np.random.rand(n_state*n_state, n_action)  # Q-Table\n",
        "\n",
        "    def quantize(self, state, n_state, w):  # 状態の値を整数のインデックスに変換\n",
        "        min = - n_state / 2 * w\n",
        "        nw = (state - min) / w\n",
        "        nw = int(nw)\n",
        "        nw = 0 if nw < 0 else nw\n",
        "        nw = n_state-1 if nw >= n_state-1 else nw\n",
        "        return nw\n",
        "\n",
        "    def train(self, states, next_states, action, next_action, reward, terminal):  # Q-Tableを訓練\n",
        "        i = self.quantize(states[0], self.n_state, self.w_y)  # 位置のインデックス\n",
        "        j = self.quantize(states[1], self.n_state, self.w_vy)  # 速度のインデックス\n",
        "        q = self.q_table[i*self.n_state+j, action]  # 現在のQ値\n",
        "\n",
        "        next_i = self.quantize(next_states[0], self.n_state, self.w_y)  # 次の位置のインデックス\n",
        "        next_j = self.quantize(next_states[1], self.n_state, self.w_vy)  # 次の速度のインデックス\n",
        "        q_next =   # ← ここにコードを追記\n",
        "\n",
        "        if terminal:\n",
        "            self.q_table[i*self.n_state+j, action] = q + self.lr*reward  # 終了時は報酬のみ使用\n",
        "        else:\n",
        "            self.q_table[i*self.n_state+j, action] = q + self.lr*(reward + self.gamma*q_next - q)  # Q値の更新式\n",
        "\n",
        "    def get_action(self, states):\n",
        "        if np.random.rand() < self.eps:  # ランダムな行動\n",
        "            action = np.random.randint(self.n_action)\n",
        "        else:  # Q値の高い行動を選択\n",
        "            i = self.quantize(states[0], self.n_state, self.w_y)\n",
        "            j = self.quantize(states[1], self.n_state, self.w_vy)\n",
        "            action = np.argmax(self.q_table[i*self.n_state+j])\n",
        "        if self.eps > 0.1:  # εの下限\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9t6fuQvDPOf"
      },
      "source": [
        "## エージェントのクラス\n",
        "SARSAに対応するために、1つ先の時間の行動もtrainメソッドに渡すようにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP4AaA5bHTCQ"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n",
        "        self.v_x = v_x  # x速度\n",
        "        self.v_y_sigma = v_y_sigma  # y速度、初期値の標準偏差\n",
        "        self.v_jump = v_jump  # ジャンプ速度\n",
        "        self.brain = brain\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = -1  # 初期x座標\n",
        "        self.y = 0  # 初期y座標\n",
        "        self.v_y = self.v_y_sigma * np.random.randn()  # 初期y速度\n",
        "        self.action = np.random.randint(2)  # 最初の行動はランダムに\n",
        "\n",
        "    def step(self, g):  # 時間を1つ進める g:重力加速度\n",
        "        states = np.array([self.y, self.v_y])\n",
        "        self.x += self.v_x\n",
        "        self.y += self.v_y\n",
        "\n",
        "        reward = 0  # 報酬\n",
        "        terminal = False  # 終了判定\n",
        "        if self.x>1.0:\n",
        "            reward = 1\n",
        "            terminal = True\n",
        "        elif self.y<-1.0 or self.y>1.0:\n",
        "            reward = -1\n",
        "            terminal = True\n",
        "        reward = np.array([reward])\n",
        "\n",
        "        if self.action == 0:\n",
        "            self.v_y -= g   # 自由落下\n",
        "        else:\n",
        "            self.v_y = self.v_jump  # ジャンプ\n",
        "        next_states = np.array([self.y, self.v_y])\n",
        "        next_action = self.brain.get_action(next_states)\n",
        "        self.brain.train(states, next_states, self.action, next_action, reward, terminal)\n",
        "        self.action = next_action\n",
        "\n",
        "        if terminal:\n",
        "            self.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr-bkx6REXCx"
      },
      "source": [
        "## 環境のクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCxzW2RHYrl"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, agent, g):\n",
        "        self.agent = agent\n",
        "        self.g = g  # 重力加速度\n",
        "\n",
        "    def step(self):\n",
        "        self.agent.step(self.g)\n",
        "        return (self.agent.x, self.agent.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hot23xovE-fj"
      },
      "source": [
        "## アニメーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuEBjtK-JNbz"
      },
      "source": [
        "def animate(environment, interval, frames):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.close()\n",
        "    ax.set_xlim(( -1, 1))\n",
        "    ax.set_ylim((-1, 1))\n",
        "    sc = ax.scatter([], [])\n",
        "\n",
        "    def plot(data):\n",
        "        x, y = environment.step()\n",
        "        sc.set_offsets(np.array([[x, y]]))\n",
        "        return (sc,)\n",
        "\n",
        "    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XhwfFz1GKOl"
      },
      "source": [
        "## SARSAの実行\n",
        "SARSAを実行し、動作を確認しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOryPVWuJTme"
      },
      "source": [
        "n_state = 10\n",
        "w_y = 0.2\n",
        "w_vy = 0.2\n",
        "n_action = 2\n",
        "brain = Brain(n_state, w_y, w_vy, n_action, r=0.99)  # εが減衰する\n",
        "\n",
        "v_x = 0.05\n",
        "v_y_sigma = 0.1\n",
        "v_jump = 0.2\n",
        "agent = Agent(v_x, v_y_sigma, v_jump, brain)\n",
        "\n",
        "g = 0.2\n",
        "environment = Environment(agent, g)\n",
        "\n",
        "anim = animate(environment, 50, 1024)\n",
        "rc(\"animation\", html=\"jshtml\")\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4NiN-L3PHtq"
      },
      "source": [
        "# 解答例\n",
        "以下に解答例を掲載します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfyY-IXAPI8j"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_state, w_y, w_vy, n_action, gamma=0.9, r=0.99, lr=0.01):\n",
        "        self.n_state = n_state  # 状態の数\n",
        "        self.w_y = w_y  # 位置の刻み幅\n",
        "        self.w_vy = w_vy  # 速度の刻み幅\n",
        "        self.n_action = n_action  # 行動の数\n",
        "\n",
        "        self.eps = 1.0  # ε\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.r = r  # εの減衰率\n",
        "        self.lr = lr  # 学習係数\n",
        "\n",
        "        self.q_table = np.random.rand(n_state*n_state, n_action)  # Q-Table\n",
        "\n",
        "    def quantize(self, state, n_state, w):  # 状態の値を整数のインデックスに変換\n",
        "        min = - n_state / 2 * w\n",
        "        nw = (state - min) / w\n",
        "        nw = int(nw)\n",
        "        nw = 0 if nw < 0 else nw\n",
        "        nw = n_state-1 if nw >= n_state-1 else nw\n",
        "        return nw\n",
        "\n",
        "    def train(self, states, next_states, action, next_action, reward, terminal):  # Q-Tableを訓練\n",
        "        i = self.quantize(states[0], self.n_state, self.w_y)  # 位置のインデックス\n",
        "        j = self.quantize(states[1], self.n_state, self.w_vy)  # 速度のインデックス\n",
        "        q = self.q_table[i*self.n_state+j, action]  # 現在のQ値\n",
        "\n",
        "        next_i = self.quantize(next_states[0], self.n_state, self.w_y)  # 次の位置のインデックス\n",
        "        next_j = self.quantize(next_states[1], self.n_state, self.w_vy)  # 次の速度のインデックス\n",
        "        q_next = self.q_table[next_i*self.n_state+next_j, next_action]  # ← ここにコードを追記\n",
        "\n",
        "        if terminal:\n",
        "            self.q_table[i*self.n_state+j, action] = q + self.lr*reward  # 終了時は報酬のみ使用\n",
        "        else:\n",
        "            self.q_table[i*self.n_state+j, action] = q + self.lr*(reward + self.gamma*q_next - q)  # Q値の更新式\n",
        "\n",
        "    def get_action(self, states):\n",
        "        if np.random.rand() < self.eps:  # ランダムな行動\n",
        "            action = np.random.randint(self.n_action)\n",
        "        else:  # Q値の高い行動を選択\n",
        "            i = self.quantize(states[0], self.n_state, self.w_y)\n",
        "            j = self.quantize(states[1], self.n_state, self.w_vy)\n",
        "            action = np.argmax(self.q_table[i*self.n_state+j])\n",
        "        if self.eps > 0.1:  # εの下限\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}